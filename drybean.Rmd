---
title: "Selecting Features in a Dry Bean Dataset"
subtitle: "Report for HarvardX PH125.9x: Data Science: Capstone"
author: "Samuel Bates"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  bookdown::pdf_document2:
    toc: FALSE
editor_options: 
  markdown: 
    wrap: 80
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
require(tidyverse)
require(readxl)
require(grid)
require(gridExtra)
require(kableExtra)
require(RColorBrewer)
require(ggridges)
require(scales)
options(digits = 5)

# Converts a vector of strings into a string containing a comma-separated list.
v2s <- function(v) paste(v, collapse = ",")

# Converts a string containing a comma-separated list into a vector of strings.
s2v <- function(s) strsplit(s, ",") %>% unlist()

beans <- read_excel("DryBeanDataset/Dry_Bean_Dataset.xlsx")
beans <- beans %>% mutate(Class = factor(str_to_title(Class)))
colnames(beans) <- c("A", "P", "X", "x", "K", "Ec", "C", "Ed", "Ex", "S", "R",
                     "CO", "SF1", "SF2", "SF3", "SF4", "Class")
features <- setdiff(colnames(beans), "Class")
fig1caption <- "Distributions of the features, grouped by the type of feature."
```

# Introduction

For this report, I am exploring a few different ways to select a subset of
features for training a classification model. I chose a dataset of measurements
performed on seven types of dry beans[@koklu2020]. I investigate the feature
distributions in a couple of ways, and create an algorithm for selecting
features that maximizes accuracy with as few features as possible. I test the
various sets of features on five different models, indicating the accuracy of
each model on each feature set. Two of the models report the importance of
features during training, so I compare them to the feature sets I construct.
Lastly, I check the accuracy of the trained models on a testing set.

# Data Exploration

The dataset contains 13,611 observations. Each observation was constructed from
a high-resolution photograph of a dry bean, and has 17 features. Twelve of the
features are geometric, four are unknown "shape factors" $SF1 - SF4$, and one
($Class$) indicates the type of bean. The twelve geometric factors are:

1.  Area (denoted by $A$): The number of pixels in the bean image.

2.  Perimeter ($P$): The length of the bean boundary measured in pixels.

3.  Major axis length ($X$[^1]): The length of the longest line that can be
    drawn within the bean boundary.

4.  Minor axis length ($x$): The length of the longest line perpendicular to the
    major axis that can be drawn within the bean boundary.

5.  Aspect ratio ($K$): Defined as $X / x$.

6.  Eccentricity ($Ec$): The eccentricity of an ellipse that has the same
    moments of inertia as the bean image.

7.  Convex area ($C$): The number of pixels within the smallest convext polygon
    that contains the bean image.

8.  Equivalent diameter ($Ed$): The diameter of a circle having the same area as
    the bean image. Expressed mathematically, $A = \pi {(Ed/2)}^2$, so $Ed$ is
    defined as $\sqrt{4 A / \pi}$.

9.  Extent ($Ex$): The ratio of the pixels in the bounding box to the bean area.

10. Solidity ($S$): Also known as convexity. The ratio of the pixels in the
    convex shell to the pixels in the bean image.

11. Roundness ($R$): Defined as $4 \pi A / P^2$.

12. Compactness ($CO$): Defined as $Ed / X$.

[^1]: The document accompanying the dataset uses $L$ and $l$ for the major and
    minor axis lengths. I changed them to $X$ and $x$, respectively, for greater
    readability.

There are seven types of bean represented: Barbunya, Bombay, Cali, Dermason,
Horoz, Seker, and Sira. Figure \@ref(fig:feature-ridgeplots) shows the
distribution of the features for each type of bean. I will use $F_{all}$ to
denote the full set of 16 features.

```{r feature-ridgeplots, fig.height=7, fig.cap=`fig1caption`}
ridges <- geom_density_ridges(lwd = .2)
no_y <- theme(axis.text.y = element_blank(),
              axis.title.y = element_blank(),
              axis.ticks.y = element_blank())
no_legend <- theme(legend.position = "none")
colrs <- scale_fill_manual(values = brewer.pal(7, "Set1"))
x_text <- theme(axis.text.x = element_text(size = 8),
                axis.title.x = element_text(family = "Times", size = 9))

p01 <- beans %>% ggplot(aes(A, y = fct_rev(Class), fill = Class)) +
  theme_minimal() + ridges + xlab("Area (A)") + 
  labs(fill = "Type of bean") + no_y + x_text + colrs +
  theme(legend.text = element_text(family = "Times"),
        legend.title = element_text(family = "Times")) +
  scale_x_continuous(labels = comma)

p07 <- beans %>% ggplot(aes(C, y = fct_rev(Class), fill = Class)) +
  theme_minimal() + ridges + xlab("Convex area (C)") + 
  no_y + no_legend + x_text + colrs +
  scale_x_continuous(labels = comma)

feature_ridgeplot <- function(featname, featdesc) {
  featsym <- sym(featname)
  beans %>% ggplot(aes(!!featsym, y = fct_rev(Class), fill = Class)) +
    theme_minimal() + no_y + xlab(sprintf("%s (%s)", featdesc, featname)) +
    ridges + no_legend + x_text + colrs
}

p02 <- feature_ridgeplot("P", "Perimeter")
p03 <- feature_ridgeplot("X", "Major axis length")
p04 <- feature_ridgeplot("x", "Minor axis length")
p05 <- feature_ridgeplot("K", "Aspect Ratio")
p06 <- feature_ridgeplot("Ec", "Eccentricity")
p08 <- feature_ridgeplot("Ed", "Equiv. diameter")
p09 <- feature_ridgeplot("Ex", "Extent")
p10 <- feature_ridgeplot("S", "Solidity")
p11 <- feature_ridgeplot("R", "Roundness")
p12 <- feature_ridgeplot("CO", "Compactness")
p13 <- feature_ridgeplot("SF1", "Shape factor 1") +
  theme(axis.text.x = element_text(size = 6))
p14 <- feature_ridgeplot("SF2", "Shape factor 2")
p15 <- feature_ridgeplot("SF3", "Shape factor 3")
p16 <- feature_ridgeplot("SF4", "Shape factor 4")

areas <- arrangeGrob(p01, p07, 
                     top = "Features measuring an area",
                     ncol = 2, widths = c(.575, .425))
distances <- arrangeGrob(p02, p03, p04, p08, 
                         top = "Features measuring a distance",
                         ncol = 4)
others <- arrangeGrob(p05, p06, p09, p10, p11, p12, p13, p14, p15, p16,
                      top = "Features measuring a ratio or shape factor",
                      ncol = 5, nrow = 2)
grid.arrange(areas, distances, others, nrow = 3, heights = c(7, 5, 8)/20)
```

## Feature selection

### Using the definitions

The first step in feature selection relies on the feature definitions
themselves: Five of the geometric features are defined in terms of other
features. Four of these are explicit: aspect ratio, equivalent diameter,
roundness, and compactness. In addition, solidity $S$ is defined in terms of the
"convex shell," which is similar to the "smallest convex polygon" used in the
definition of $C$. Experimentally, I found that $S$ is equal to $A/C$. These
five relationships are verified to 9 decimal places by the following code:

```{r math-relations, echo=TRUE, message=TRUE}
near_zero <- 10^-10
all(
  max(abs(beans$K - beans$X / beans$x)) < near_zero,
  max(abs(beans$A - pi * beans$Ed^2 / 4)) < near_zero,
  max(abs(beans$S - beans$A / beans$C)) < near_zero,
  max(abs(beans$R - 4 * pi * beans$A / beans$P^2)) < near_zero,
  max(abs(beans$CO - beans$Ed / beans$X)) < near_zero)
```

Consequently, an observation is completely defined by just 11 features: the 7
remaining geometric features plus the 4 shape factors.

### Using correlation

Correlation allows me to eliminate another feature. Figure
\@ref(fig:feature-ridgeplots) shows that the area $A$ and convex area $C$
distributions for each type of bean are very similar. Table
\@ref(tab:correlations) shows that they are highly correlated, so $C$ can be
safely removed from the list of features. The table provides additional evidence
that I may safely remove $CO$ and $Ed$, as $SF3$ and $P$, respectively, are
highly correlated with them. I will use $F_{min}$ to denote the resulting set of
10 features $\{ A, P, X, x, Ec, Ex, SF1, SF2, SF3, SF4 \}$.

```{r correlations}
corr_table <- map_dfr(1:(length(features) - 1), function(i) {
  map_dfr((i+1):length(features), function(j) {
    tibble(f1 = features[i], f2 = features[j],
           corr = cor(beans %>% select(all_of(f1)),
                      beans %>% select(all_of(f2))))
  })
})
high_corrs <- corr_table %>% arrange(desc(abs(corr))) %>% filter(abs(corr) > .99)
knitr::kable(high_corrs, booktabs = T, digits = 5,
             col.names = c("Feature 1", "Feature 2", "Correlation"),
             caption = "The most highly-correlated features.") %>%
  kable_styling(latex_options = "hold_position")
```

Principal component analysis suggests that $F_{min}$ may still be larger than
necessary. I applied `prcomp` to the set of features and discovered that six
components can explain 100% of the variability. In fact, the first two
components can explain the variablity to 6 decimal places; adding four more
components reduces the standard deviation to less than 1 (Table \@ref(tab:pca)).

```{r pca}
pca <- prcomp(beans %>% select(-Class))
options(scipen = 5)
knitr::kable(summary(pca)$importance[, 1:6], booktabs = T,
             caption = "The first six components of principal component analysis.")
```

# Models and Analysis

I tested five models on the training set using both the full and minimal sets of
features:

-   linear discriminant analysis (`lda`);

-   quadratic discriminant analysis (`qda`);

-   k nearest neighbors (`knn`);

-   recursive partitioning (`rpart2`); and

-   bootstrap aggregated trees (`treebag`).

The result appears in Figure \@ref(fig:base-accuracies). In three of the models
(`lda`, `qda`, and `treebag`), the minimal set $F_{min}$ produced a lower
accuracy than the full set $F_{all}$. The other two models (`knn` and `rpart2`)
showed $F_{min}$ producing as high an accuracy or higher than that produced by
$F_{all}$. This demonstrates that the goal of feature selection is not a vain
one.

```{r setcaption-base}
fig_caption_base <- 
  "Accuracy of each model and feature set on the training set."
```

```{r base-accuracies, fig.cap=`fig_caption_base`}
training_accuracies <- readRDS("training_accuracies.rds")

accuracy_colplot <- function(dataset, colrs) {
  dataset %>% ggplot(aes(accuracy, name, fill = name)) +
    geom_col() +
    geom_text(aes(label = round(accuracy, 5)), hjust = 1.2) +
    theme_minimal() + xlab("Accuracy") + ylab("Feature sets") +
    scale_fill_manual(values = colrs) +
    theme(axis.text = element_text(family = "Times"),
          axis.title = element_text(family = "Times"),
        legend.position = "none")
}

bar_colors <- brewer.pal(4, "Pastel1")[2:4]
curr_colors <- rep(bar_colors[1], 2)
accuracy_colplot(training_accuracies, curr_colors) +
  scale_y_discrete(labels = c(expression(italic(F[min])), 
                              expression(italic(F[all])))) + 
  facet_wrap(. ~ model, ncol = 1)

```

## Feature selection by repeated predictions

To further explore what features are truly necessary, I wrote an algorithm that
uses repeated predictions to find a minimal set of features that maximizes
accuracy. The algorithm proceeds as follows:

1.  Using a feature set $F$, calculate the accuracy of a model on each single
    feature $f$ in $F$. Choose a feature with the highest accuracy (there may be
    more than one). Say it is feature $f_1$, and that the accuracy is $A_1$. I
    denote the set of used features with $U$; in this case, it is $\{ f_1 \}$. I
    denote the accuracy of the model on $U$ with $A_U$.

2.  For each feature $g$ that is in $F - U$, calculate the accuracy $A_g$ of the
    model on the set of features $U \cup \{ g \}$. (After Step 1, this is the
    set $\{ f_1, g \}$.)

    1.  If all the values in $\{ A_g | g \in F - U \}$ are less than or equal to
        $A_U$, then stop.

    2.  If there is a value $A_g > A_U$, pick a feature with the largest value
        (again, there may be more than one) and add it to the set $U$. Reset
        $A_U$ to be the new higher value $A_g$.

3.  Repeat Step 2 until either there are no features left (i.e., $U = F$), or no
    feature produces a higher accuracy than $A_U$. The final set $U$ is the
    desired minimal set of features.

The algorithm will clearly produce different minimal sets for different models.
Less obviously, it will produce different minimal sets for different values of
$F$. Figure \@ref(fig:fewest-features) shows the results of running the
algorithm on each model with $F = F_{all}$ or $F = F_{min}$. The accuracies for
the full set and minimal set are shown for comparison.

```{r setcaption-fewest}
fig_caption_fewest <- 
  sprintf("%s %s %s %s %s", 
          "Accuracy of each model on the two feature sets",
          "and on the minimal sets produced by the algorithm",
          "(denoted by $Few(F_{all})$ and $Few(F_{min})$).",
          "The feature sets discovered by the algorithm are",
          "shown at the base of the corresponding bar.")
```

```{r fewest-features, fig.cap=`fig_caption_fewest`, fig.height=5.5}
fewest_all <- readRDS("fewest_all.rds")
fewest_min <- readRDS("fewest_min.rds")
bar_data <- rbind(training_accuracies %>% mutate(label = ""), 
                  fewest_all, fewest_min)

curr_colors <- c(rep(bar_colors[2], 2), rep(bar_colors[1], 2))
accuracy_colplot(bar_data, curr_colors) +
  geom_text(aes(x = 0.01, label = label, family = "Times", fontface = "italic"),
            hjust = 0) +
  scale_y_discrete(labels = c(expression(italic(Few(F[min]))), 
                              expression(italic(Few(F[all]))), 
                              expression(italic(F[min])), 
                              expression(italic(F[all])))) + 
  facet_wrap(. ~ model, ncol = 1)
```

## Feature selection by model-dependent importance

One last exploration involves the idea of variable importance. When the caret
package `train` function is executed on the last two models, `rpart2` and
`treebag`, the result includes a table showing the importance of each variable
or feature for predicting. I compared the tables to the feature sets produced by
my algorithm; the results appear in Tables \@ref(tab:alg-vs-imp-1) -
\@ref(tab:alg-vs-imp-4). Each table shows the importance of each feature as a
percentage, with importance decreasing from left to right. The highlighted
columns indicate the features found by the Few algorithm. I also calculated the
accuracy of the two models on all features with at least 50% importance; the
results are shown in Figure \@ref(fig:important-features).

```{r alg-vs-imp-1}
imp_table <- function(mod, which, table_caption) {
  # Find the set of features found by the algorithm.
  fewest <- eval(sym(sprintf("fewest_%s", which)))
  fewest_fset <- fewest %>% filter(model == mod) %>% pull(label) %>% s2v()
  
  # Find the set of features (variables) reported as important by the model.
  var_imp_table <- readRDS(sprintf("%s_var_imp_%s.rds", mod, which))
  display_table <- var_imp_table %>% arrange(desc(Overall)) %>% t()
  rownames(display_table) <- NULL
  
  # Mark the columns in the latter set that match the former set.
  match_fewest <- which(colnames(display_table) %in% fewest_fset)
  
  # Show the latter set with highlighted columns for the former set.
  if (length(colnames(display_table)) < 12) {
  knitr::kable(display_table, booktabs = T,
               caption = table_caption) %>%
    column_spec(match_fewest, bold = TRUE, background = "yellow",
                include_thead = TRUE)
  } else {
  knitr::kable(display_table, booktabs = T,
               caption = table_caption) %>%
    column_spec(match_fewest, bold = TRUE, background = "yellow",
                include_thead = TRUE) %>%
      kable_styling(latex_options = "scale_down")
  }
}
rpart2_caption1 <- 
  sprintf("%s %s %s %s",
          "Importance of features according to the rpart2 model when",
          "trained on $F_{all}$.",
          "The highlighted columns indicate the set of features discovered",
          "by the algorithm.")
imp_table("rpart2", "all", rpart2_caption1)
```

```{r alg-vs-imp-2}
rpart2_caption2 <- 
  sprintf("%s %s %s %s",
          "Importance of features according to the rpart2 model when",
          "trained on $F_{min}$.",
          "The highlighted columns indicate the set of features discovered",
          "by the algorithm.")
imp_table("rpart2", "min", rpart2_caption2)
```

```{r alg-vs-imp-3}
treebag_caption1 <- 
  sprintf("%s %s %s %s",
          "Importance of features according to the treebag model when",
          "trained on $F_{all}$.",
          "The highlighted columns indicate the set of features discovered",
          "by the algorithm.")
imp_table("treebag", "all", treebag_caption1)
```

```{r alg-vs-imp-4}
treebag_caption2 <- 
  sprintf("%s %s %s %s",
          "Importance of features according to the treebag model when",
          "trained on $F_{all}$.",
          "The highlighted columns indicate the set of features discovered",
          "by the algorithm.")
imp_table("treebag", "min", treebag_caption2)
```

```{r setcaption_important}
fig_caption_imp <- sprintf(
  "%s %s %s",
  "Accuracies of different sets of features on the rpart2 and treebag",
  "models. The bars marked *Important* show the accuracy on the set of",
  "features that has at least 50% importance according to the respective model."
)
```

```{r important-features, fig.cap=`fig_caption_imp`, fig.height=4}
fewest_all <- readRDS("fewest_all.rds")
fewest_min <- readRDS("fewest_min.rds")
importance_accuracies <- readRDS("importance_accuracies.rds")
bar_data <- rbind(training_accuracies %>% mutate(label = ""), 
                  fewest_all, fewest_min, importance_accuracies) %>%
  filter(model %in% c("rpart2", "treebag"))

curr_colors <- c(rep(bar_colors[2], 2), 
                 rep(bar_colors[3], 2), 
                 rep(bar_colors[1], 2))
accuracy_colplot(bar_data, curr_colors) +
  geom_text(aes(x = 0.01, label = label, family = "Times", fontface = "italic"),
            hjust = 0) +
  scale_y_discrete(labels = c(expression(italic(Few(F[min]))), 
                              expression(italic(Few(F[all]))), 
                              expression(italic(Important(F[all]))), 
                              expression(italic(Important(F[min]))), 
                              expression(italic(F[min])), 
                              expression(italic(F[all])))) + 
  facet_wrap(. ~ model, ncol = 1)
```

# Results

```{r improvements}
train_all <- training_accuracies %>% filter(name == "whole")
train_min <- training_accuracies %>% filter(name == "min")
loss_max <- min(train_min$accuracy/train_all$accuracy)
gain_max <- max(train_min$accuracy/train_all$accuracy)
percent_change <- function(x) round(1 - x, 3) * 100


```

The initial feature selection, using the definitions and correlation, reduced
accuracy only slightly (a maximum loss of `r percent_change(loss_max)`%)
compared to using the full feature set. It even improved accuracy by
`r -percent_change(gain_max)`% in the `knn` model. The feature sets found by the
algorithm improved accuracy in every case compared to the full feature set.
Furthermore, the largest feature set found by the algorithm had only 7 features
and all other discovered feature sets had either 4 or 5 features. This appears
to corroborate the dimensionality suggested by the principal component analysis.

On the other hand, the features reported as important by the `rpart2` and
`treebag` models did not improve accuracy. The features found by the algorithm
showed no particular relation to the important features. In fact, two of the
features found by the algorithm for `rpart2` have an importance of zero (Table
\@ref(tab:alg-vs-imp-1)).

With the exception of the `treebag` model, the accuracy results on the testing
set are very similar to the accuracy results on the training set. The accuracy
results for the `treebag` model were very high on the training set, suggesting
that it may have been overtrained.

```{r setcaption-test}
fig_caption_test <- sprintf(
  "%s",
  "Accuracy of each model and feature set on the testing set."
)

```

```{r fig-test, fig.cap=`fig_caption_test`,fig.height=5.5}
test_accuracies <- readRDS("test_accuracies.rds")
accuracy_colplot(test_accuracies, c(rep(bar_colors[2], 2), 
                                    rep(bar_colors[1], 2))) +
  geom_text(aes(x = 0.01, label = label, family = "Times", fontface = "italic"),
            hjust = 0) +
  scale_y_discrete(labels = c(expression(italic(Few(F[min]))), 
                              expression(italic(Few(F[all]))), 
                              expression(italic(F[min])), 
                              expression(italic(F[all])))) + 
  facet_wrap(. ~ model, ncol = 1)

```

# Conclusions

Judicious feature selection can clearly improve the accuracy of some models. My
algoriththm is an attempt to identify the most important features for a model.
It has one major disadvantage: it may increase the training time significantly.
In the worst case, where every feature is significant, the training time is
multiplied by $N^2/2$, where $N$ is the number of features.

# Bibliography
