---
title: "Selecting Features in a Dry Bean Dataset"
subtitle: "Report for HarvardX PH125.9x: Data Science: Capstone"
author: "Samuel Bates"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 80
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
require(tidyverse)
require(readxl)
require(grid)
require(gridExtra)
require(kableExtra)
require(RColorBrewer)
require(ggridges)
require(scales)
options(digits = 5)

# Converts a vector of strings into a string containing a comma-separated list.
v2s <- function(v) paste(v, collapse = ",")

# Converts a string containing a comma-separated list into a vector of strings.
s2v <- function(s) strsplit(s, ",") %>% unlist()

beans <- read_excel("DryBeanDataset/Dry_Bean_Dataset.xlsx")
beans <- beans %>% mutate(Class = factor(str_to_title(Class)))
colnames(beans) <- c("A", "P", "X", "x", "K", "Ec", "C", "Ed", "Ex", "S", "R",
                     "CO", "SF1", "SF2", "SF3", "SF4", "Class")
features <- setdiff(colnames(beans), "Class")
fig1caption <- "Distributions of the features, grouped by the type of feature."
```

# Introduction

For this report, I am exploring a few different ways to select a subset of
features for training a classification model. I chose a dataset of measurements
performed on seven types of dry beans[@koklu2020].

Applied two techniques to select features.

Trained five models on four sets of features to see which could most accurately
predict the type of bean from the features.

Constructed a mechanism for selecting a minimal set of features that maximizes
the accuracy of a model.

One of the models reports the importance of each feature in its classification.
I compare the sets of important features to the sets resulting from the minimal
set construction. Finally, I calculate the accuracy of the four model on the
four sets of features using the reserved test set.

# Data Exploration

The dataset contains 13,611 observations. Each observation was constructed from
a high-resolution photograph of a dry bean, and has 17 features. Twelve of the
features are geometric, four are unknown "shape factors" $SF1 - SF4$, and one
($Class$) indicates the type of bean. The twelve geometric factors are:

1.  Area (denoted by $A$): The number of pixels in the bean image.

2.  Perimeter ($P$): The length of the bean boundary measured in pixels.

3.  Major axis length ($X$[^1]): The length of the longest line that can be
    drawn within the bean boundary.

4.  Minor axis length ($x$): The length of the longest line perpendicular to the
    major axis that can be drawn within the bean boundary.

5.  Aspect ratio ($K$): Defined as $X / x$.

6.  Eccentricity ($Ec$): The eccentricity of an ellipse that has the same
    moments of inertia as the bean image.

7.  Convex area ($C$): The number of pixels within the smallest convext polygon
    that contains the bean image.

8.  Equivalent diameter ($Ed$): The diameter of a circle having the same area as
    the bean image. Expressed mathematically, $A = \pi {(Ed/2)}^2$, so $Ed$ is
    defined as $\sqrt{4 A / \pi}$.

9.  Extent ($Ex$): The ratio of the pixels in the bounding box to the bean area.

10. Solidity ($S$): Also known as convexity. The ratio of the pixels in the
    convex shell to the pixels in the bean image.

11. Roundness ($R$): Defined as $4 \pi A / P^2$.

12. Compactness ($CO$): Defined as $Ed / X$.

[^1]: The document accompanying the dataset uses $L$ and $l$ for the major and
    minor aix lengths. I changed it to X's since a lower-case L is not always
    distinguishable from an upper-case I.

There are seven types of bean represented: Barbunya, Bombay, Cali, Dermason,
Horoz, Seker, and Sira. Figure 1 shows the distribution of the features for each
type of bean. I will use $F_{all}$ to denote the full set of 16 features

```{r feature-ridgeplots, fig.height=7, fig.cap=`fig1caption`}
ridges <- geom_density_ridges(lwd = .2)
no_y <- theme(axis.text.y = element_blank(),
              axis.title.y = element_blank(),
              axis.ticks.y = element_blank())
no_legend <- theme(legend.position = "none")
colrs <- scale_fill_manual(values = brewer.pal(7, "Set1"))
x_text <- theme(axis.text.x = element_text(size = 8),
                axis.title.x = element_text(family = "Times", size = 9))

p01 <- beans %>% ggplot(aes(A, y = fct_rev(Class), fill = Class)) +
  theme_minimal() + ridges + xlab("Area (A)") + 
  labs(fill = "Type of bean") + no_y + x_text + colrs +
  theme(legend.text = element_text(family = "Times"),
        legend.title = element_text(family = "Times")) +
  scale_x_continuous(labels = comma)

p07 <- beans %>% ggplot(aes(C, y = fct_rev(Class), fill = Class)) +
  theme_minimal() + ridges + xlab("Convex area (C)") + 
  no_y + no_legend + x_text + colrs +
  scale_x_continuous(labels = comma)

feature_ridgeplot <- function(featname, featdesc) {
  featsym <- sym(featname)
  beans %>% ggplot(aes(!!featsym, y = fct_rev(Class), fill = Class)) +
    theme_minimal() + no_y + xlab(sprintf("%s (%s)", featdesc, featname)) +
    ridges + no_legend + x_text + colrs
}

p02 <- feature_ridgeplot("P", "Perimeter")
p03 <- feature_ridgeplot("X", "Major axis length")
p04 <- feature_ridgeplot("x", "Minor axis length")
p05 <- feature_ridgeplot("K", "Aspect Ratio")
p06 <- feature_ridgeplot("Ec", "Eccentricity")
p08 <- feature_ridgeplot("Ed", "Equiv. diameter")
p09 <- feature_ridgeplot("Ex", "Extent")
p10 <- feature_ridgeplot("S", "Solidity")
p11 <- feature_ridgeplot("R", "Roundness")
p12 <- feature_ridgeplot("CO", "Compactness")
p13 <- feature_ridgeplot("SF1", "Shape factor 1") +
  theme(axis.text.x = element_text(size = 6))
p14 <- feature_ridgeplot("SF2", "Shape factor 2")
p15 <- feature_ridgeplot("SF3", "Shape factor 3")
p16 <- feature_ridgeplot("SF4", "Shape factor 4")

areas <- arrangeGrob(p01, p07, 
                     top = "Features measuring an area",
                     ncol = 2, widths = c(.575, .425))
distances <- arrangeGrob(p02, p03, p04, p08, 
                         top = "Features measuring a distance",
                         ncol = 4)
others <- arrangeGrob(p05, p06, p09, p10, p11, p12, p13, p14, p15, p16,
                      top = "Features measuring a ratio or shape factor",
                      ncol = 5, nrow = 2)
grid.arrange(areas, distances, others, nrow = 3, heights = c(7, 5, 8)/20)
```

## Feature selection

The first step in feature selection is in the feature definitions themselves:
Five of the geometric features are defined in terms of other features. Four of
these are explicit: aspect ratio, equivalent diameter, roundness, and
compactness. In addition, solidity $S$ is defined in terms of the "convex
shell," which is similar to the "smallest convex polygon" used in the definition
of $C$. Experimentally, I found that $S$ is equal to $A/C$. These five
relationships are verified to 9 decimal places by the following code:

```{r math-relations, echo=TRUE, message=TRUE}
near_zero <- 10^-10
all(
  max(abs(beans$K - beans$X / beans$x)) < near_zero,
  max(abs(beans$A - pi * beans$Ed^2 / 4)) < near_zero,
  max(abs(beans$S - beans$A / beans$C)) < near_zero,
  max(abs(beans$R - 4 * pi * beans$A / beans$P^2)) < near_zero,
  max(abs(beans$CO - beans$Ed / beans$X)) < near_zero)
```

Consequently, an observation is completely defined by just 11 features: the 7
remaining geometric features plus the 4 shape factors.

Correlation allows me to eliminate another feature. Figure 1 shows that the area
$A$ and convex area $C$ distributions for each type of bean are very similar.
Table 1 shows that they are highly correlated, so $C$ can be safely removed from
the list of features. The table provides additionaly evidence that I may safely
remove $CO$ and $Ed$, as $SF3$ and $P$, respectively, are highly correlated with
them. I will use $F_{min}$ to denote the resulting set of 10 features
$\{ A, P, X, x, Ec, Ex, SF1, SF2, SF3, SF4 \}$.

```{r correlations}
corr_table <- map_dfr(1:(length(features) - 1), function(i) {
  map_dfr((i+1):length(features), function(j) {
    tibble(f1 = features[i], f2 = features[j],
           corr = cor(beans %>% select(all_of(f1)),
                      beans %>% select(all_of(f2))))
  })
})
high_corrs <- corr_table %>% arrange(desc(abs(corr))) %>% filter(abs(corr) > .99)
knitr::kable(high_corrs, booktabs = T, digits = 5,
             col.names = c("Feature 1", "Feature 2", "Correlation"),
             caption = "The most highly-correlated features.") %>%
  kable_styling(latex_options = "hold_position")
```

Principal component analysis suggests that $F_{min}$ may still be larger than
necessary. I applied `prcomp` to the set of features and discovered that six
components can explain 100% of the variability. In fact, the first two
components can explain the variablity to 6 decimal places; adding four more
components reduces the standard deviation to less than 1 (Table 2).

```{r pca}
pca <- prcomp(beans %>% select(-Class))
options(scipen = 5)
knitr::kable(summary(pca)$importance[, 1:6], booktabs = T,
             caption = "The first six components of principal component analysis.")
```

# Models and Analysis

I tested five models on the training set using both the full and minimal sets of
features:

-   linear discriminant analysis (`lda`);

-   quadratic discriminant analysis (`qda`);

-   k nearest neighbors (`knn`);

-   a CART model (`rpart2`); and

-   bootstrap aggregated trees (`treebag`).

The result appears in Figure 2. In three of the models, the minimal set
$F_{min}$ produced a lower accuracy than the full set $F_{all}$. The other two
models (`knn` and `rpart2`) showed $F_{min}$ producing as high an accuracy or
higher than that produced by $F_{all}$. This demonstrates that the goal of
feature selection is not a vain one.

```{r base_accuracies, fig.cap="Accuracy of each model on both feature sets."}
training_accuracies <- readRDS("training_accuracies.rds")
training_accuracies %>% ggplot(aes(accuracy, name)) +
  geom_col(fill = I("turquoise")) +
  geom_text(aes(label = round(accuracy, 5)), hjust = 1.2) +
  theme_minimal() + ylab("Feature sets") + xlab("Accuracy") +
  theme(axis.text = element_text(family = "Times")) +
  scale_y_discrete(labels = c(expression(italic(F[min])), 
                              expression(italic(F[all])))) + 
  facet_wrap(. ~ model, ncol = 1)

```

To further explore what features are truly necessary for accurate prediction, I
wrote an algorithm that uses repeated predictions to find a minimal set of
features that maximizes accuracy. The algorithm proceeds as follows:

1.  Using a feature set $F$, calculate the accuracy of a model on each single
    feature $f$ in $F$. Choose a feature with the highest accuracy (there may be
    more than one). Say it is feature $f_1$, and that the accuracy is $A_1$. I
    denote the set of used features with $U$; in this case, it is $\{ f_1 \}$. I
    denote the accuracy of the model on $U$ with $A_U$.

2.  For each feature $g$ that is in $F - U$, calculate the accuracy $A_g$ of the
    model on the set of features $U \cup \{ g \}$. (After Step 1, this is the
    set $\{ f_1, g \}$.)

    1.  If all the values in $\{ A_g | g \in F - U \}$ are less than or equal to
        $A_U$, then stop.

    2.  If there is a value $A_g > A_U$, pick a feature with the largest value
        (again, there may be more than one) and add it to the set $U$. Reset
        $A_U$ to be the new higher value $A_g$.

3.  Repeat Step 2 until either there are no features left (i.e., $U = F$), or no
    feature produces a higher accuracy than $A_U$. The final set $U$ is the
    desired minimal set of features.

The algorithm will clearly produce different minimal sets for different models.
Less obviously, it will produce different minimal sets for different values of
$F'$. Figure 3 shows the results of running the algorithm on each model with
$F = F_{all}$ or $F = F_{min}$. The accuracies for the full set and minimal set
are shown for comparison.

```{r setcaption3}
fig3caption <- sprintf("%s %s %s %s", 
                       "Accuracy of each model on the two feature sets",
                       "and on the minimal sets produced by the algorithm.",
                       "The feature sets discovered by the algorithm are",
                       "shown at the base of the corresponding bar.")
```

```{r fewest_features, fig.cap=`fig3caption`, fig.height=5.5}
fewest_all <- readRDS("fewest_all.rds")
fewest_min <- readRDS("fewest_min.rds")
bar_data <- rbind(training_accuracies %>% mutate(label = ""), 
                  fewest_all, fewest_min)
bar_data %>% ggplot(aes(accuracy, name)) +
  geom_col(fill = I("turquoise")) +
  geom_text(aes(label = round(accuracy, 5)), hjust = 1.2) +
  geom_text(aes(x = 0.01, label = label, family = "Times"), hjust = 0) +
  theme_minimal() + ylab("Feature sets") + xlab("Accuracy") +
  theme(axis.text = element_text(family = "Times")) +
  scale_y_discrete(labels = c(expression(italic(Few(F[min]))), 
                              expression(italic(Few(F[all]))), 
                              expression(italic(F[min])), 
                              expression(italic(F[all])))) + 
  facet_wrap(. ~ model, ncol = 1)
```

One last exploration involves the idea of variable importance. When the caret
package `train` function is executed on the last two models, `rpart2` and
`treebag`, the result includes a table showing the importance of each variable
or feature for predicting. I compared the lists to the feature sets produced by
my algorithm; the result is in Tables 3 - 6.

```{r alg_vs_importance}
rpart2_fset_few_all <- fewest_all %>% filter(model == "rpart2") %>%
  pull(label) %>% s2v()
rpart2_var_imp_all <- readRDS("rpart2_var_imp_all.rds")
rpart2_all_table <- rpart2_var_imp_all %>% arrange(desc(Overall)) %>% t()
rownames(rpart2_all_table) <- NULL
matches <- which(colnames(rpart2_all_table) %in% rpart2_fset_few_all)
knitr::kable(rpart2_all_table, booktabs = T,
             caption = "rpart2 on full set") %>%
  column_spec(matches, bold = TRUE, background = "yellow", 
              include_thead = TRUE) %>%
  kable_styling(latex_options = "scale_down")

rpart2_fset_few_min <- fewest_min %>% filter(model == "rpart2") %>%
  pull(label) %>% s2v()
rpart2_var_imp_min <- readRDS("rpart2_var_imp_min.rds")
rpart2_min_table <- rpart2_var_imp_min %>% arrange(desc(Overall)) %>% t()
rownames(rpart2_min_table) <- NULL
matches <- which(colnames(rpart2_min_table) %in% rpart2_fset_few_min)
knitr::kable(rpart2_min_table, booktabs = T,
             caption = "rpart2 on minimal set") %>%
  column_spec(matches, bold = TRUE, background = "yellow", 
              include_thead = TRUE)
```

# Results

Tables and figures yet to do:

-   (Figure) Accuracy of `rpart2` and `treebag` models on four sets above plus
    $Important(F_{all})$ and $Important(F_{min})$Â .

-   (2 Tables) Overlap of $Few(F)$ and $Important(F)$ feature sets for `treebag`
    model.

-   (Figure) Accuracy of all models on $F_{all}$, $F_{min}$, $Few(F_{all})$, and
    $Few(F_{min})$ on the test set.

Improvements:

-   Change colors of bars to differentiate base fsets from Few fsets from
    Important fsets.

# Conclusions

x

# Bibliography
